---
title: "Models with Memory"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 500,
  fig.width = 9,
  fig.height = 6
)
```

* Thus far, all models have used dummy or indicator variables, implictly making the assumption that there's nothing to be learned from one category to another.
* We want, instead, to be able to learn how categories are different while also learning how they may be similar!
* *Multilevel models* help in this regard. Here are some benefits:
  1. **Improved estimates for repeat sampling**: When there are more than one observation from the same individual, location, or time, traditional, single-level models either maximally underfit or overit the data.
  2. **Improved estimates for imbalance in sampling**: When some individuals, locations, or times are sampled more than others, multilvel models automatically cope with differing uncertainty (i.e., over-sampled clusters don't dominate inference unfairly). 
  3. **Estimates of variation**: Multilevel models model variation within and between groups explicitly.
  4. **Avoid averaging, retain variation**: Summarising at a roll-up level with an average is dangerous, since it removes variation!
  
## 13.1 Example: Multilevel tadpoles

```{r}
library(rethinking)

# frogs!
data(reedfrogs)
d <- reedfrogs
str(d)
```

* Let's model the number surviving, `surv`, out of an initial count, `density`. 
* Each row is a tank containing tadpoles, so let's create a *varying intercept* model based on each tank.
* As a comparison point, let's start with a categorical model.

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(0, 1.5)
\end{gather}
$$

```{r}
# make the tank cluster variable
d$tank <- 1:nrow(d)

# prep for stan
dat <-
  list(
    S = d$surv,
    N = d$density,
    tank = d$tank
  )

# approximate posterior
m13.1 <-
  ulam(
    alist(S ~ dbinom(N, p),
          logit(p) <- a[tank],
          a[tank] ~ dnorm(0, 1.5)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )

precis(m13.1, depth = 2)
```

* Here is nothing new --- we have 48 different estimates of alpha, one for each tank. 
* Let's do the multilevel version:

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma \sim \text{Exponential}(1)
\end{gather}
$$

* Now, each tank intercept, $\alpha_j$, is a function of two parameters: $\overline{\alpha}$ and $\sigma$. There are two levels for $\alpha$ (hence, the name "multilevel"). 
* The two parameters, $\overline{\alpha}$ and $\sigma$ are often referred to as *hyperparameters*, and their priors, *hyperpriors*. 
* In principle, there is no limit to the number of "hypers" or levels we can model, but in practice there are computational limits and limits in our ability to understand the model.
* We can fit this model with `ulam()`, but not with `quap()`! Since `quap()` just approximates posteriors by "climbing a hill," it can't infer the posterior across multiple levels (there is a more robust explanation later). 

```{r}
# multilevel tadpoles!
m13.2 <-
  ulam(
    alist(S ~ dbinom(N, p),
          logit(p) <- a[tank],
          a[tank] ~ dnorm(a_bar, sigma),
          a_bar ~ dnorm(0, 1.5),
          sigma ~ dexp(1)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )
```

```{r}
compare(m13.1, m13.2)
```

* `m13.2` has only ~21 effective parameters! The prior shrinks all the intercept estimates towards the mean $\overline{\alpha}$. 
* This is despite the model having more actual parameters (50) than `m13.1` (48).

```{r}
precis(m13.2)
```

* `sigma` is a regularizing prior, like from earlier chapters, but now the amount of regularization has been learned from the model itself!

```{r}
# extract samples
post <- extract.samples(m13.2)

# find mean for each tank
# and transform to log-probability
d$propsurv.est <- logistic(apply(post$a, 2, mean))

# display raw proportions surviving in each tank
plot(
  d$propsurv,
  ylim = c(0, 1),
  pch = 16,
  xaxt = "n",
  xlab = "tank",
  ylab = "proportion survival",
  col = rangi2
)

axis(1, at=c(1, 16, 32, 48), labels = c(1, 16, 32, 48))

# overlay posterior means
points(d$propsurv.est)

# mark posterior mean probability across tanks
abline(h = mean(inv_logit(post$a_bar)), lty = 2)

# draw vertical dividers between tank densities
abline(v = 16.5, lwd = 0.5)
abline(v = 32.5, lwd = 0.5)
text(8, 0, "small tanks")
text(16 + 8, 0, "medium tanks")
text(32 + 8, 0, "large tanks")
```

* In every case, the multilevel estimate (open) is closer to the prior mean (dashed line) than the raw estimates (blue). This is called *shrinkage*. 
* The smaller tanks (with fewer tadpoles) also shrink back towards the group mean more than the tanks with many tadpoles.
* Shrinkage is also proportional to how far away from the group mean the estimate is --- the further away, the greater the shrinkage. 
* All of these arise because of *pooling* --- sharing information across groups.

```{r}
# show first 100 populations in the posterior
plot(
  NULL,
  xlim = c(-3, 4),
  ylim = c(0, 0.35), 
  xlab = "log-odds survive",
  ylab = "Density"
)

for (i in 1:100) {
  
  curve(dnorm(x, post$a_bar[i], post$sigma[i]), add = TRUE, col = col.alpha("black", 0.2))
  
}

# sample 8000 imaginary tanks from the posterior distribution
sim_tanks <- rnorm(8000, post$a_bar, post$sigma)

# transform to probability and visualize
dens(inv_logit(sim_tanks), lwd = 2, adj = 0.1)
```

* Thus far, exponential priors on $\sigma$ terms have worked well, and they often continue to work well in multilevel models. There are, however, sometimes times when there are too few clusters to estimate variance from the max-entropy exponential distribution, so a half-normal may be more appropriate, i.e.:

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma \sim \text{Half-Normal}(0, 1)
\end{gather}
$$

* This can be done in `ulam()` with `dhalfnorm()` with a parameter for the lower bound of 0: `lower = 0`. 

