---
title: "Models with Memory"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 500,
  fig.width = 9,
  fig.height = 6
)
```

* Thus far, all models have used dummy or indicator variables, implictly making the assumption that there's nothing to be learned from one category to another.
* We want, instead, to be able to learn how categories are different while also learning how they may be similar!
* *Multilevel models* help in this regard. Here are some benefits:
  1. **Improved estimates for repeat sampling**: When there are more than one observation from the same individual, location, or time, traditional, single-level models either maximally underfit or overit the data.
  2. **Improved estimates for imbalance in sampling**: When some individuals, locations, or times are sampled more than others, multilvel models automatically cope with differing uncertainty (i.e., over-sampled clusters don't dominate inference unfairly). 
  3. **Estimates of variation**: Multilevel models model variation within and between groups explicitly.
  4. **Avoid averaging, retain variation**: Summarising at a roll-up level with an average is dangerous, since it removes variation!
  
## 13.1 Example: Multilevel tadpoles

```{r}
library(rethinking)

# frogs!
data(reedfrogs)
d <- reedfrogs
str(d)
```

* Let's model the number surviving, `surv`, out of an initial count, `density`. 
* Each row is a tank containing tadpoles, so let's create a *varying intercept* model based on each tank.
* As a comparison point, let's start with a categorical model.

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(0, 1.5)
\end{gather}
$$

```{r}
# make the tank cluster variable
d$tank <- 1:nrow(d)

# prep for stan
dat <-
  list(
    S = d$surv,
    N = d$density,
    tank = d$tank
  )

# approximate posterior
m13.1 <-
  ulam(
    alist(S ~ dbinom(N, p),
          logit(p) <- a[tank],
          a[tank] ~ dnorm(0, 1.5)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )

precis(m13.1, depth = 2)
```

* Here is nothing new --- we have 48 different estimates of alpha, one for each tank. 
* Let's do the multilevel version:

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma \sim \text{Exponential}(1)
\end{gather}
$$

* Now, each tank intercept, $\alpha_j$, is a function of two parameters: $\overline{\alpha}$ and $\sigma$. There are two levels for $\alpha$ (hence, the name "multilevel"). 
* The two parameters, $\overline{\alpha}$ and $\sigma$ are often referred to as *hyperparameters*, and their priors, *hyperpriors*. 
* In principle, there is no limit to the number of "hypers" or levels we can model, but in practice there are computational limits and limits in our ability to understand the model.
* We can fit this model with `ulam()`, but not with `quap()`! Since `quap()` just approximates posteriors by "climbing a hill," it can't infer the posterior across multiple levels (there is a more robust explanation later). 

```{r}
# multilevel tadpoles!
m13.2 <-
  ulam(
    alist(S ~ dbinom(N, p),
          logit(p) <- a[tank],
          a[tank] ~ dnorm(a_bar, sigma),
          a_bar ~ dnorm(0, 1.5),
          sigma ~ dexp(1)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )
```

```{r}
compare(m13.1, m13.2)
```

* `m13.2` has only ~21 effective parameters! The prior shrinks all the intercept estimates towards the mean $\overline{\alpha}$. 
* This is despite the model having more actual parameters (50) than `m13.1` (48).

```{r}
precis(m13.2)
```

* `sigma` is a regularizing prior, like from earlier chapters, but now the amount of regularization has been learned from the model itself!

```{r}
# extract samples
post <- extract.samples(m13.2)

# find mean for each tank
# and transform to log-probability
d$propsurv.est <- logistic(apply(post$a, 2, mean))

# display raw proportions surviving in each tank
plot(
  d$propsurv,
  ylim = c(0, 1),
  pch = 16,
  xaxt = "n",
  xlab = "tank",
  ylab = "proportion survival",
  col = rangi2
)

axis(1, at=c(1, 16, 32, 48), labels = c(1, 16, 32, 48))

# overlay posterior means
points(d$propsurv.est)

# mark posterior mean probability across tanks
abline(h = mean(inv_logit(post$a_bar)), lty = 2)

# draw vertical dividers between tank densities
abline(v = 16.5, lwd = 0.5)
abline(v = 32.5, lwd = 0.5)
text(8, 0, "small tanks")
text(16 + 8, 0, "medium tanks")
text(32 + 8, 0, "large tanks")
```

* In every case, the multilevel estimate (open) is closer to the prior mean (dashed line) than the raw estimates (blue). This is called *shrinkage*. 
* The smaller tanks (with fewer tadpoles) also shrink back towards the group mean more than the tanks with many tadpoles.
* Shrinkage is also proportional to how far away from the group mean the estimate is --- the further away, the greater the shrinkage. 
* All of these arise because of *pooling* --- sharing information across groups.

```{r}
# show first 100 populations in the posterior
plot(
  NULL,
  xlim = c(-3, 4),
  ylim = c(0, 0.35), 
  xlab = "log-odds survive",
  ylab = "Density"
)

for (i in 1:100) {
  
  curve(dnorm(x, post$a_bar[i], post$sigma[i]), add = TRUE, col = col.alpha("black", 0.2))
  
}

# sample 8000 imaginary tanks from the posterior distribution
sim_tanks <- rnorm(8000, post$a_bar, post$sigma)

# transform to probability and visualize
dens(inv_logit(sim_tanks), lwd = 2, adj = 0.1)
```

* Thus far, exponential priors on $\sigma$ terms have worked well, and they often continue to work well in multilevel models. There are, however, sometimes times when there are too few clusters to estimate variance from the max-entropy exponential distribution, so a half-normal may be more appropriate, i.e.:

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{TANK[i]} \\
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma \sim \text{Half-Normal}(0, 1)
\end{gather}
$$

* This can be done in `ulam()` with `dhalfnorm()` with a parameter for the lower bound of 0: `lower = 0`. 

## 13.2 Varying effects and the underfitting/overfitting trade-off

* Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while also estimating the features of each cluster.
* Varying effects provide more accurate estimates of the cluster intercepts. This is because they do a better job of trading off between overfitting/underfitting. 
* Let's look at predicting the survival of frogs from several ponds using a few different methods:
  1. Complete pooling --- assume the population does not vary at all from pond to pond.
  2. No pooling --- assume that each pond tells us nothing about any other pond. 
  3. Partial pooling --- using an adaptive regularizing prior (like the last section). 
* Complete pooling will underfit the data, since the estimate for $\alpha$ across all ponds is unlikely to fit any particular pond well.
* No pooling will overfit the data, since there is little data about each pond in particular.
* Partial pooling strikes a balance!

### 13.2.1 The model

* We'll be simulating data from this model, then use each strategy to see how well it recovers the parameters:

$$
\begin{gather}
S_i \sim \text{Binomial}(N_i, p_i) \\
\text{logit}(p_i) = \alpha_{POND[i]} \\
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma \sim \text{Exponential}(1)
\end{gather}
$$

* We'll need to assign values to:
  * $\overline{\alpha}$, the average log-odds of survival in the entire population of ponds. 
  * $\sigma$, the standard deviation of the distribution of log-odds of survival among ponds.
  * $\alpha$, a vector of individual pond intercepts, one for each pond. 
  * $N_i$, a sample size for each pond.
  
### 13.2.2 Assign values to the parameters

```{r}
# specify parameters
a_bar <- 1.5
sigma <- 1.5
nponds <- 60
Ni <- as.integer(rep(c(5, 10, 25, 35), each = 15))

# setup df
set.seed(5005)
a_pond <- rnorm(nponds, mean = a_bar, sd = sigma)
d_sim <- data.frame(pond = 1:nponds, Ni = Ni, true_a = a_pond)
```

* We've used `as.integer()` when creating `Ni`. R doesn't care too much about this being a `"numeric"` rather than an `"integer"`, but Stan does!

### 13.2.3 Simulate survivors

```{r}
d_sim$Si <- rbinom(nponds, prob = logistic(d_sim$true_a), size = d_sim$Ni)
```

### 13.2.4 Compute the no-pooling estimates

```{r}
d_sim$p_nopool <- d_sim$Si / d_sim$Ni
```

* The no pooling estimate we've added here is the same thing we'd get if we'd fit a model with flat priors that induce no regularization. 

### 13.2.5 Compute the partial-pooling estimate

```{r}
# prep for stan
dat <- 
  list(
    Si = d_sim$Si, 
    Ni = d_sim$Ni, 
    pond = d_sim$pond
  )

# model!
m13.3 <-
  ulam(
    alist(Si ~ dbinom(Ni, p),
          logit(p) <- a_pond[pond],
          a_pond[pond] ~ dnorm(a_bar, sigma),
          a_bar ~ dnorm(0, 1.5),
          sigma ~ dexp(1)),
    data = dat,
    chains = 4
  )

# display output
precis(m13.3, depth = 2)

# extract means for comparison
post <- extract.samples(m13.3)
d_sim$p_partpool <- apply(inv_logit(post$a_pond), 2, mean)

# true per-pond survival probabilities
d_sim$p_true <- inv_logit(d_sim$true_a)

# compute error between estimates and true effects
nopool_error <- abs(d_sim$p_nopool - d_sim$p_true)
partpool_error <- abs(d_sim$p_partpool - d_sim$p_true)

# plot!
plot(
  1:60, 
  nopool_error,
  xlab = "pond",
  ylab = "absolute error",
  col = rangi2,
  pch = 16
)

points(1:60, partpool_error)
```

* The blue points are the no-pooling estimates, the black circles show the varying effect estimates. 
* At the high end, the partially pooled/no pooled estimates are similar, but at the low end, the partially pooled estimates are way better!

```{r}
nopool_avg <- aggregate(nopool_error, list(d_sim$Ni), mean)
partpool_avg <- aggregate(partpool_error, list(d_sim$Ni), mean)

nopool_avg

partpool_avg
```

* Smaller tanks (with less information) are shrunk more towards the mean than large tanks (with lots more info). 

## 13.3 More than one type of cluster

* We can (and often should) include more than one type of cluster in a model.
* Let's look back at the chimpanzee lever-pulling data --- each chimp is its own cluster, but there are also clusters of experimental groups.
* The data structure in `data(chimpanzees)` is *cross-classified*, since actors are not nested within unique blocks. The model specification will still be the same for MCMC.

### 13.3.1 Multilevel chimpanzees

* Let's add varying intercepts to our chimpanzee model (recall that we had 4 treatment cases):

$$
\begin{gather}
L_i \sim \text{Binomial}(1, p_i) \\
\text{logit}(p_i) = \alpha_{ACTOR[i]} + \gamma_{BLOCK[i]} + \beta_{TREATMENT[i]} \\
\beta_j \sim \text{Normal}(0, 0.5) \\ 
\alpha_j \sim \text{Normal}(\overline{\alpha}, \sigma_{\alpha}) \\
\gamma_j \sim \text{Normal}(0, \sigma_{\gamma}) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma_{\alpha} \sim \text{Exponential}(1) \\
\sigma_{\gamma} \sim \text{Exponential}(1)
\end{gather}
$$

* Note that there is only one global mean variable, $\overline{\alpha}$. We can't identify a separate mean for each varying intercept type, since both are added to the same linear prediction. 
* Doing so won't be the end of the world, but would be like the right/left leg example from Chapter 6. 

```{r}
# load data
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

# prep for stan
dat_list <-
  list(
    pulled_left = d$pulled_left,
    actor = d$actor,
    block_id = d$block,
    treatment = as.integer(d$treatment)
  )

# model!
set.seed(13)
m13.4 <-
  ulam(
    alist(
      # model
      pulled_left ~ dbinom(1, p),
      logit(p) <- a[actor] + g[block_id] + b[treatment],
      
      # prior
      b[treatment] ~ dnorm(0, 0.5),
      
      # adaptive priors
      a[actor] ~ dnorm(a_bar, sigma_a),
      g[block_id] ~ dnorm(0, sigma_g),
      
      # hyper-priors
      a_bar ~ dnorm(0, 1.5),
      sigma_a ~ dexp(1),
      sigma_g ~ dexp(1)
    ),
    
    data = dat_list,
    chains = 4,
    cores = 4,
    log_lik = TRUE
  )
```

* In the book, McElreath expected 22 divergent transitions. McElreath notes that this is fine, but the sampler had some trouble efficiently exploring the posterior. We'll fix this later.
* Let's explore the model --- this is the most complicated model we've put together so far:

```{r}
precis(m13.4, depth = 2)
precis_plot(precis(m13.4, depth = 2))
```

* A couple things worth noting:
  1. `n_eff` varies quite a lot across parameters. This is common in complex models and may be a result of the sampler spending a lot of time near a boundary for one parameter (here, that's `sigma_g` spending too much time near 0). 
  2. Compare `sigma_a` to `sigma_g` --- the estimated variation among actors is a lot larger than the estimated variation among blocks. The chimps vary, but the blocks are all the same. 

```{r}
# fit a model that ignores block
set.seed(14)
m13.5 <-
  ulam(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a[actor] + b[treatment],
          b[treatment] ~ dnorm(0, 0.5),
          a[actor] ~ dnorm(a_bar, sigma_a),
          a_bar ~ dnorm(0, 1.5),
          sigma_a ~ dexp(1)),
    data = dat_list,
    chains = 4,
    cores = 4,
    log_lik = TRUE
  )

# compare to the model with both clusters
compare(m13.4, m13.5)
```

* Here, the `pWAIC` column reports the effective number of parameters. `m13.4` has 7 more parameters than `m13.5`, but only 2 more effective parameters, because the posterior distribution for `sigma_g` ended up close to 0.
* This means that each of the 6 `g` parameters are pretty inflexible, while the `a` parameters resulted in less shrinkage. 
* The difference in `WAIC` between the models is small --- the block parameters contribute little additional information to the model.
* There is nothing to gain by selecting either model, but the comparison tells a rich story: whether we include block or not hardly matters, and the `g` and `sigma_g` parameters tell us why. 

### 13.3.2 Even more clusters

* Let's fit again with a varying effect for treatment (this will piss off certain folks with a specific background in statistical semantics). 

```{r}
set.seed(15)
m13.6 <-
  ulam(
    alist(
      # model
      pulled_left ~ dbinom(1, p),
      logit(p) <- a[actor] + g[block_id] + b[treatment],
      
      # adaptive priors
      b[treatment] ~ dnorm(0, sigma_b),
      a[actor] ~ dnorm(a_bar, sigma_a),
      g[block_id] ~ dnorm(0, sigma_g),
      
      # hyper-priors
      a_bar ~ dnorm(0, 1.5),
      sigma_a ~ dexp(1),
      sigma_g ~ dexp(1),
      sigma_b ~ dexp(1)
    ),
    
    data = dat_list,
    chains = 4, 
    cores = 4,
    log_lik = TRUE
  )

precis(m13.6, depth = 2)
precis_plot(precis(m13.6, depth = 2))
compare(m13.4, m13.6)
```

* Here, we don't get a whole lot of additonal info by setting the intercept to vary by treatment --- we do get divergent transitions though. Let's deal with those.

## 13.4 Divergent transitions and non-centered priors

* Divergent transitions are commonplace in multilevel models (don't I know!). 
* Recall that HMC simulates each parameter as a particle in a physics system. In this system, the total energy should be conserved, but sometimes the calculation shows different energy values between the start/end of a step.
* This a divergence --- it tends to happen when the posterior is very steep in some region of the parameter space. 
* Divergent transitions are rejected --- they don't directly damage the approximation of the posterior, but they do hurt it indirectly, since the region where the divergence occurred is hard to explore correctly.
* There are two easy tricks to reducing the impact of divergent transitions:
  1. Amp up Stan's `adapt_delta` parameter --- do more warmup with a higher target acceptance rate.
  2. *Reparameterize* the model.
  
### 13.4.1 The Devil's Funnel

* Let's look at an example of a joint distribution with two variables:

$$
\begin{gather}
\nu \sim \text{Normal}(0, 3) \\
x \sim \text{Normal}(0, \text{exp}(\nu))
\end{gather}
$$

* Here's a basic fit of this joint distribution with `ulam()`

```{r}
m13.7 <-
  ulam(
    alist(v ~ normal(0, 3),
          x ~ normal(0, exp(v))),
    data = list(N = 1),
    chains = 4
  )

precis(m13.7)
```

* This looks like an easy problem with only two parameters, but we get loads of divergent transitions!

```{r}
traceplot(m13.7)
```

* This is an example of The Devil's Funnel --- at low values of $\nu$, the distribution of $x$ contracts around 0 in a very steep valley (see figure 13.5 on page 422). 
* Steep surfaces are difficult to simulate, because the simulation isn't continuous but instead happens in discrete steps. If the steps are too big, the simulation will overshoot. 
* Here, we can reparameterize to fix!
* The distribution of $x$ is conditional on $\nu$, so we'd call this a *centered parameterization*.
* The *non-centered parameterization* of this would move the embedded parameter, $\nu$, out of the definition of the other parameter. 

$$
\begin{gather}
\nu \sim \text{Normal}(0, 3) \\
z \sim \text{Normal}(0, 1) \\
x = z \ \text{exp}(\nu)
\end{gather}
$$

* Now, when we run this through HMC, instead of sampling $x$ directly, the sampler draws from $\nu$ and $z$, which is much easier to sample from (just two normal distributions!). The right hand side of figure 13.5 on page 422 shows this.

```{r}
m13.7nc <-
  ulam(
    alist(v ~ normal(0, 3),
          z ~ normal(0, 1),
          gq> real[1]:x <<- z*exp(v)),
    data = list(N = 1),
    chains = 4
  )

precis(m13.7nc)
```

* This is much better! Non-centered parameterizations are often helpful for multilevel models, but there are times when centered parameterizations are better, so it's beneficial to be familiar with both. 

### 13.4.2 Non-centered chimpanzees

* For a real example, the adaptive priors in `m13.4` make it a multilevel model with parameters inside of them. This is causing regions of steep curvatures and divergent transitions that we can fix by non-centering.
* Before attempting to fix with reparameterization, we ought to try increasing the `adapt_delta` control.
* Higher `adapt_delta` means a smaller step size, so we may more accurately approximate a curved surface.

```{r}
set.seed(13)
m13.4b <-
  ulam(
    m13.4,
    chains = 4,
    cores = 4,
    control = list(adapt_delta = 0.99)
  )

divergent(m13.4b)
```

* That knocked down the number of divergent transitions, but didn't get rid of them all. And the `n_eff` is pretty low (around 300 when we sampled 2000 times!):

```{r}
precis(m13.4b, depth = 2)
```

* The non-centered version can do much better! There are three centered/embedded parameters to smuggle out of the priors: $\overline{\alpha}$, $\sigma_{\alpha}$, and $\sigma_{\gamma}$. 

$$
\begin{gather}
L_i \sim \text{Binomial}(1, p_i) \\
\text{logit}(p_i) = \underbrace{\overline{\alpha} + z_{\text{ACTOR}[i]} \ \sigma_\alpha}_{\alpha_{\text{ACTOR}[i]}} + \underbrace{x_{\text{BLOCK}[i]} \ \sigma_\gamma}_{\gamma_{\text{BLOCK}[i]}} + \beta_{\text{TREATMENT}[i]} \\
\beta_j \sim \text{Normal}(0, 0.5) \\
z_j \sim \text{Normal}(0, 1) \\
x_j \sim \text{Normal}(0, 1) \\
\overline{\alpha} \sim \text{Normal}(0, 1.5) \\
\sigma_\alpha \sim \text{Exponential}(1) \\
\sigma_\gamma \sim \text{Exponential}(1)
\end{gather}
$$

* Here, the vector $z$ gives the standardized intercept for each actor and the vector $x$ gives the standardized intercept for each block.
* Inside the linear model, all the parameters reappear, but the actor/block intercepts are no longer sampled from embedded parameters!

```{r}
set.seed(13)
m13.4nc <-
  ulam(
    alist(
      # model, nc!
      pulled_left ~ dbinom(1, p),
      logit(p) <- a_bar + z[actor]*sigma_a + # actor intercepts
                  x[block_id]*sigma_g +      # block intercepts
                  b[treatment],
      
      # prior
      b[treatment] ~ dnorm(0, 0.5),
      
      # adaptive priors
      z[actor] ~ dnorm(0, 1),
      x[block_id] ~ dnorm(0, 1),
      
      # hyper-priors
      a_bar ~ dnorm(0, 1.5),
      sigma_a ~ dexp(1),
      sigma_g ~ dexp(1),
      
      # generated quantiles
      gq> vector[actor]:a <<- a_bar + z*sigma_a,
      gq> vector[block_id]:g <<- x*sigma_g
    ),
    
    data = dat_list,
    chains = 4,
    cores = 4
  )
```

* Now, the `n_eff` is generally better:

```{r}
precis(m13.4nc, depth = 2)

# get neff for centered/noncentered params in a single table
precis_c <- precis(m13.4, depth = 2)
precis_nc <- precis(m13.4nc, depth = 2)
pars <- c(paste0("a[", 1:7, "]"),
          paste0("g[", 1:6, "]"),
          paste0("b[", 1:4, "]"),
          "a_bar",
          "sigma_a",
          "sigma_g")

neff_table <- cbind(precis_c[pars, "n_eff"], precis_nc[pars, "n_eff"])

# plot
plot(
  neff_table,
  xlim = range(neff_table),
  ylim = range(neff_table),
  xlab = "n_eff (centered)",
  ylab = "n_eff (non-centered)",
  lwd = 2
)

abline(a = 0, b = 1, lty = 2)
```

* For all but two parameters, the n_eff is greater for the non-centered parameterization. 
* Typically, for a cluster with low variation like the blocks in `m13.4`, the non-centered version will perform better. 
* If you have a large number of units inside a cluster, but not much data for each unit, then the non-centered is also usually better. 
* Being able to switch between the two, however, is useful!
* You can also obviously reparameterize distributions other than the Gaussian.

## 13.5 Multilevel posterior predictions 

* Producing implied predictions from a model's fit is, once again, a useful endeavor. 
* Implied predictions are needed to consider causal effects.
* Another role for constructing implied predictions is for computing *information criteria*, like AIC or WAIC, which provide estimates of out-of-sample accuracy (via KL divergence). 

### 13.5.1 Posterior prediction for same clusters

* When working with the same clusters that were used to fit a model, varying intercepts are just parameters. 
* For multilevel models, we shouldn't expect the model to retrodict the data used to train it (this is actually the whole point of partial pooling --- shrinking estimates towards a global mean when there is little data guards us from poor out-of-sample predictions). 
* Let's compute posterior predictions for actor 2 using `m13.4`

```{r}
# setup data
chimp <- 2
d_pred <-
  list(
    actor = rep(chimp, 4),
    treatment = 1:4,
    block_id = rep(1, 4)
  )

# extract mean samples using link
p <- link(m13.4, data = d_pred)
p_mu <- apply(p, 2, mean)
p_ci <- apply(p, 2, PI)
```

* Alternatively, we can perform the same calculation manually using the posterior samples.

```{r}
# posterior samples
post <- extract.samples(m13.4)
str(post)

# posterior density for actor 5
dens(post$a[,5])

# posterior prediction function
p_link <- function(treatment, actor = 1, block_id = 1) {
  
  logodds <- 
    with(
      post,
      a[,actor] + g[,block_id] + b[,treatment]
    )
  
  return(inv_logit(logodds))
  
}

p_raw <- sapply(1:4, function(i) p_link(i, actor = 2, block_id = 1))
p_mu <- apply(p_raw, 2, mean)
p_ci <- apply(p_raw, 2, PI)
```

* At some point, you'll run into a model that `link()` won't be able to handle correctly. You can always work directly with the samples to get posterior predictions out. 

### 13.5.2 Posterior prediction for new clusters

* Let's suppose you want to predict how chimpanzees in another population would respond to the lever pulling experiment. 
* The actor intercepts don't give you what you need, since those intercepts are for the particular chimps from the original experiment. 
* One way to think about this is to generate predictions for a new, previously unobserved, *average* actor (aka, a chimpanzee with an intercept exactly at `a_bar`, $\overline{\alpha}$). 
* There will still be uncertainty around this "average" chimp, but it will be misleadingly smaller than it really should be. 

```{r}
p_link_abar <- function(treatment) {
  
  logodds <- with(post, a_bar + b[,treatment])
  return(inv_logit(logodds))
  
}
```

* This link ignores `block`, since we are extrapolating to new blocks and making the assumption that the average block effect is zero. 

```{r}
post <- extract.samples(m13.4)
p_raw <- sapply(1:4, function(i) p_link_abar(i))
p_mu <- apply(p_raw, 2, mean)
p_ci <- apply(p_raw, 2, PI)

plot(
  NULL,
  xlab = "treatment",
  ylab = "proportion pulled left",
  ylim = c(0, 1),
  xaxt = "n",
  xlim = c(1, 4)
)

axis(
  1, 
  at = 1:4, 
  labels = c("R/N", "L/N", "R/P", "L/P")
)

lines(1:4, p_mu)
shade(p_ci, 1:4)
```

* This shows the variation around the *average* actor --- to show the variation among individual (unobserved) actors, we'll also need to use `sigma_a` $\sigma_{\alpha}$ in the calculation.

```{r}
a_sim <- 
  with(
    post,
    rnorm(length(post$a_bar), a_bar, sigma_a)
  )

p_link_asim <- function(treatment) {
  
  logodds <- with(post, a_sim + b[,treatment])
  return(inv_logit(logodds))
  
}

p_raw_asim <- sapply(1:4, function(i) p_link_asim(i))

plot(
  NULL,
  xlab = "treatment",
  ylab = "proportion pulled left",
  ylim = c(0, 1),
  xaxt = "n",
  xlim = c(1, 4)
)

axis(
  1, 
  at = 1:4,
  labels = c("R/N", "L/N", "R/P", "L/P")
)

for (i in 1:100) lines(1:4, p_raw_asim[i,], col = grau(0.25), lwd = 2)
```

* Here, each trend is a simulated actor --- the uncertainty around individual actor outputs is far greater than the "average" actor output!
* Now the question is, which is better? Well, it depends.
* The predictions for an average actor help visuazlize the impact of treatment. 
* The predictions that are marginal of actor illustrate how variable different chimpanzees are, according to the model. 

### 13.5.3 Post-stratification

* A common problem is how to use a non-representative sample of a population to generate representative predictions from the same population.
* For example, we may survey voters about their voting intentions. One subgroup may respond at a greater rate than others, so our survey average will reflect this response bias.
* *Post-stratification* helps address --- the idea is to fit a model and estimate the voting intention for each slice of a demographic cake (i.e., race, age, educational attainment), then re-weight the estimated voting intentions based on general census information. 
* When combined with multilevel modeling, this is called *Multilevel Regression and Post-Stratification*/*MRP* (or "Mister P"). 
* Let's say we have estimates $p_i$ for each demographic category $i$. The post-stratified prediction for the whole population is just the reweighting of the number of individuals in the general population:

$$
\begin{gather}
\frac{\sum_i N_i p_i}{\sum_i N_i}
\end{gather}
$$

* Post-stratification doesn't always work. Say, for example, selection bias itself is caused by the outcome of interest.
* For example, we can work with a variable like age --- if age $A$ influences whether or not someone responds $R$ and is also associated with voting intention $V$, then we can still estimate the influence of $A$ on $V: \ R \leftarrow A \rightarrow V$.
* On the other hand, if $V \rightarrow R$, there is little hope. 
* Say only supporters respond --- then $V = 1$ for all respondents!
* A general framework for this sort of generalizing to a population is *transportability* --- post-stratification is a special case of this framework.
* See *External validity: From do-calculus to transportability across populations* for more.

## 13.6 Summary

* This chapter introduced the motivation, implementation, and interpretation of basic multilevel models. 
* We focused just on varying intercepts which can be thought of as pooling information within each cluster.
* The next chapter extends these concepts to additional types of parameters and models. 
