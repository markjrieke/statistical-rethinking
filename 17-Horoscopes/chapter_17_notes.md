Horoscopes
================

- Statistics books (this one included) tend to be somewhat like
  horoscopes: in order to remain plausibly correct, they must be
  general/vague enough to apply to all cases and there is a tendency for
  astrologers/statisticians to exaggerate the power/importance of their
  advice.
- Errors in science is not additive (a “sin” in one area is not atoned
  by a “virtue” in another) — everything interacts.
- Editor in chief of the *The Lancet*, Richard Horton, wrote in 2015:

> The case against science is straightforward: much of the scientific
> literature, perhaps half, may simply be untrue. Afflicted by studies
> with small sample sizes, tiny effects, invalid exploratory analyses,
> and flagrant conflicts of interest, together with an obsession for
> pusruing fashionable trends of dubious importance, science has taken a
> turn towards darkness.

- McElreath’s horoscopic advice:

> If I should offer you horoscopic advice, this is what I’d say.
> Thinking generatively — how the data could arise — solves many
> problems. Many statistical problems cannot be solved with statistics.
> All variables are measured with error. Conditioning on variables
> creates as many problems as it solves. There is no inference without
> assumption, but do not choose your assumptions for the sake of
> inference. Build complex models one piece at a time. Be critical. Be
> kind.

- There *is* a reproducibility crisis in science — even among papers in
  the best journals, it is difficult to repeat many published findings.
- The over-indexing on null hypothesis significance testing yields a
  high false discovery rate.
- The history of the sciences is (and always has been) equal parts
  wonder and blunder.
- There were, for example, many more “discovered” elements than we
  actually have on the periodic table, and they weren’t all from
  frauds/cranks.
- Here are some of McElreath’s most salient pieces of the dynamic of
  scientific discovery:

1.  **Quality of theory and predictions**: If most theories are wrong,
    most findings will be false positives. Better theories with precise
    predictions, precise tests, and more than one model is usually
    necessary.
2.  **Dynamics of funding**: The process of funding research is open to
    individual biases and may overlook important long-term research
    projects.
3.  **Quality of measurement**: Research design matters. Poor
    signal-to-noise ratios will not mean no findings, just unreliable
    ones.
4.  **Quality of data analysis**: This is the topic of this book, but
    it’s still a broader topic. Many common practices exacerbate the
    rate of false discovery — if you are not designing your analysis
    before you see the data, then your analysis may overfit the data in
    ways that regularization cannot reliably address.
5.  **Quality of peer review**: Many mistakes make it through peer
    review and many brilliant papers do not. Honestly admitting the
    limitations of work only hurts a paper’s chances, which is not good.
6.  **Publication**: We agonize over bias in measurement and statistical
    analysis, but then allow it all back in during publication.
7.  **Post-publication peer review**: What happens to a finding *after*
    publication is just as important as what happens before. Invalid
    analyses are all-too-often published in top-tier journals then torn
    apart on blogs, but there is no system for linking published papers
    to criticism. *Even retracted papers continue to be cited*.
8.  **Replication and meta-analysis**: No single study is definitive —
    the most important aspects of science are repitition and synthesis,
    but incentives to replicate and summarize are weaker than incentives
    to produce novel findings.

- We focus on the statistical analysis quite a bit more than these other
  areas because it’s one area where we have a lot of control.
- Another area that we have control over is openness. Open sourcing the
  methods, code, data, and results so that reviewers can access them
  without your interaction (i.e., don’t put “data available upon
  request” then never respond to requests) is ideal (obviously, there
  are cases, like those involving PII, that cannot be released, but most
  science isn’t like that).
- Thoughtful science is preferable to rote machinistic science. Open
  science is preferable to closed science. Thoughtful, open science is
  preferable to all.
