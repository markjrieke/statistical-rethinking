---
title: "Geocentric Models"
output: github_document
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 500,
  fig.width = 9,
  fig.height = 6
)
```

* Ptolomey's geocentric model of the solar system using epicycles could provide useful information about the future location of the planets, but was wrong.
* *Linear regression* is our geocentric golem --- it's often not necessarily correct, but can be useful.

## 4.1 Why normal distributions are normal

### 4.1.1 Normal by addition

* Let's simulate 16 random steps forward/backward for 1000 people.
* Even though these are effectively coin flips, we'll end up with a normal-ish distribution of of final position at step 16 for these 1000 people.

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))

hist(pos)
plot(density(pos))
```

* Any process that adds together random values from the same distribution converges to a normal distribution.

### 4.1.2 Normal by multiplication

* Let's the growth rate of an organism is determined by a a dozen loci that interact, such that each increase growth by a percentage. This means the effects multiply. Here's an example of 1:

```{r}
prod(1 + runif(12, 0, 0.1))
```

* Now let's repeat this for 10,000 organisms:

```{r}
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
rethinking::dens(growth, norm.comp = TRUE)
```

* This is because small multiplications are approximately additive (e.g., 1.1 * 1.1 = 1.21)

```{r}
big <- replicate(10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))

rethinking::dens(big)
rethinking::dens(small)
```

### 4.1.3 Normal by log-multiplication

* Large deviates that are multiplied together don't produce gaussian distributions, but they do generate log-normal distributions

```{r}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
rethinking::dens(log.big)
```

### 4.1.4 Using Gaussian distributions

* There are two justifications for using Gaussian distributions:
  - **Ontological**: The world is full of Gaussian distributions. Other members of the *Exponential Family* of distributions (Gamma, Poisson) also arise in nature. 
  - **Epistemological**: When all we know or are willing to say about a distribution is their mean and variance, then ghe Gaussian is the most natural way to express this. This is also justified with information theory via measures of *maximum entropy*.
  
## 4.2 A language for describing models

1. We have a set of variables to work with. Some of these are observable --- we call these *data*. Others are unobservable --- we call these *parameters*. 
2. We define each variable either in terms of other variables or in terms of a probability distribution.
3. The combination of variables and their probability distributions defines a *joint generative model*. 

* The biggest difficulty in working with this framework is the subject matter --- which variables matter and how does theory tell us to connect them?

### 4.2.1 Re-describing the globe tossing model

$$
\begin{align*}
W \sim Binomial(N, p) \\
\\
p \sim Uniform(0, 1)\\
\end{align*}
$$

* Both lines in the above model are *stochastic* --- that is no single variable on the left is known with certainty, but rather is described probabilistically. 

## 4.3 Gaussian model of height

### 4.3.1 The data

```{r}
library(rethinking)
data("Howell1")
d <- Howell1

str(d)
```

```{r}
precis(d)
```

```{r}
d2 <- d[d$age >= 18,]
```

### 4.3.2 The model

```{r}
hist(d2$height)
```

* Can model height using a normal distribution
* Notation note: *iid* = *independent and identically distributed*. This assumption is often wrong (for example, heights amongst family members are not independent), but also often useful.

$$
h_{i} \sim Normal(\mu, \sigma) \\
\mu \sim Normal(178, 20) \\
\sigma \sim Uniform(0, 50)
$$

```{r}
# ***average*** height somewhere between 140 & 220 cm
curve(dnorm(x, 178, 20), 
      from = 100,
      to = 250)
```

```{r}
curve(dunif(x, 0, 50),
      from = -10,
      to = 60)
```

```{r}
# prior predictive simulation
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

* Prior prediction is useful for setting reasonable priors. 
* For example, if the prior for the mean, $\mu$, was $\mu \sim Normal(178, 100)$, we'd end up with implausible (negative!) prior heights:

```{r}
# this set of priors is unreasonable!
sample_mu <- rnorm(1e4, 178, 100)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

* In this particular case, we have enough data and a simple enough model that a sill prior is harmless. But that won't always be the case. 

### 4.3.3 Grid approximation of the posterior distribution

* For posterity, let's generate the analytical solution to the posterior (won't be useful in the long term, but approximations will be far quicker/less computationally expensive/just as good for practical purposes):

```{r}
# generate grid of mu/sigma
mu.list <- seq(from = 150, to = 160, length.out = 100)
sigma.list <- seq(from = 7, to = 9, length.out = 100)

# generate every combination of mu/sigma (10000 total)
post <- expand.grid(mu = mu.list, sigma = sigma.list)

# generate log-likelihood of data based on each combination of mu/sigma
post$LL <- 
  sapply(1:nrow(post),
         function(i) sum(
           dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE)
         ))

# compute the log product of the likelihood & prior
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)

# find the posterior, convert from log to response scale
post$prob <- exp(post$prod - max(post$prod))
```

```{r}
contour_xyz(post$mu, post$sigma, post$prob)
```

```{r}
image_xyz(post$mu, post$sigma, post$prob)
```

### 4.3.4 Sampling from the posterior

* Let's sample parameter values from the posterior distribution

```{r}
# sample.rows = row indices --- rows with greater plausibility are more likely 
# to be sampled
sample.rows <- 
  sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)

sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

plot(sample.mu,
     sample.sigma,
     cex = 1,
     pch = 16,
     col = col.alpha(rangi2, 0.1))
```

```{r}
hist(sample.mu)
hist(sample.sigma)
```

```{r}
PI(sample.mu)
PI(sample.sigma)
```

* One interesting thing to note is that $\sigma$ is *right-skewed*. 
* This *basically* has to do with the fact that it must be positive --- if variance is estimated to be near zero, then it can't be much smaller, but could be a lot bigger.

```{r}
# using a sample of 20 points will display this issue further
d3 <- sample(d2$height, size = 20)

# repeat analysis from above, this time looking at only 20 points
# priors:
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)

# combine all possible combinations
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)

# log likelihood
post2$LL <- 
  sapply(
    1:nrow(post2),
    function(i) 
      sum(dnorm(d3, mean = post2$mu[i], sd = post2$sigma[i], log = TRUE))
  )

# log product of likelihood & prior
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) + dunif(post2$sigma, 0, 50, TRUE)

# log posterior converted to response scale
post2$prob <- exp(post2$prod - max(post2$prod))

# sample from the posterior
sample2.rows <- sample(1:nrow(post2), size = 1e4, replace = TRUE, prob = post2$prob)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]

plot(sample2.mu, 
     sample2.sigma, 
     cex = 1, 
     col = col.alpha(rangi2, 0.1),
     xlab = "mu",
     ylab = "sigma",
     pch = 16)
```

```{r}
dens(sample2.sigma, norm.comp = TRUE)
```

### 4.3.5 Finding the posterior distribution with `quap`

* `quap()` will allow us to make a *quadratic approximation* of the posterior.
* `quap()` will find the peak at the *maximum a posteriori* estimate (MAP), then approximate the posterior using the curvature at the MAP.
* This is similar to what many non-Bayesian procedures do, just without any priors.
* `quap()` allows us to define the formulas very similarly to the mathematical syntax:

$$
h_{i} \sim Normal(\mu, \sigma) \\
\mu \sim Normal(178, 20) \\
\sigma \sim Uniform(0, 50)
$$

```{r}
flist <-
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 20),
    sigma ~ dunif(0, 50)
  )
```

* Very simple to fit the model to the data:

```{r}
m4.1 <- quap(flist, data = d2)

precis(m4.1)
```

* The table from `precis()` provides the Gaussian approximations for each parameter's *marginal* distribution. 
* This means the plausibility of each value of $\mu$ after averaging over the plausibilities of each value of $\sigma$ is given by a Gaussian distribution with mean 154.6 & std. dev of 0.4.
* `quap()` will start at random parameter values by default, but you can also give it an explicit starting value:

```{r}
start <- 
  list(
    mu = mean(d2$height),
    sigma = sd(d2$height)
  )

# use start values specified above
m4.1 <- quap(flist, data = d2, start = start)

# quap still finds pretty much the same estimates:
precis(m4.1)
```

* Previous priors were very weak --- we can also specify very stron priors:

```{r}
m4.2 <-
  quap(
    alist(
      height ~ dnorm(mu, sigma),
      mu ~ dnorm(178, 0.1), # very strong prior!!!
      sigma ~ dunif(0, 50)
    ),
    data = d2
  )

precis(m4.2)
```

* Strong priors regularize *a lot* so the estimate for $\mu$ has hardly moved. 
* Even though $\mu$ hasn't moved, $\sigma$ has *had* to move quite a bit to compensate. 

### 4.3.6 Sampling from a `quap`

* Quadratic approximation is just a multidimensional gaussian ($\mu$ and $\sigma$ both contribute a dimension) distribution.
* Just like a mean and standard deviation are enough to describe a one-dimensional Gaussian distribution, a list of means and a matrix of variances and covariances are enough to describe a multidimensional Gaussian distribution.

```{r}
vcov(m4.1)
```

* `vcov()` returns a *variance-covariance* matrix which tells us how each parameter relates to every other parameter in the posterior distribution.
* A variance-covariance matrix can be factored into two elements:
  1. A vector of variances for the parameters.
  2. A correlation matrix that tells us how changes in any parameter lead to correlated changes in the others.
  
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

* The two element vector returned by `diag()` is a list of variances (if you take the square root, you get the standard deviations that are shown by `precis()`).
* Instead of sampling single values from a Gaussian, we sample vectors of values from a multi-dimensional gaussian.

```{r}
post <- extract.samples(m4.1, n = 1e4)

head(post)
precis(post)
plot(post)
```

## 4.4 Linear prediction

* Thus far, we've only modeled height. Oftentimes, we want measure how an outcome is related to some other variable (e.g., predictors).
* For example, height and weight are positvely associated:

```{r}
plot(d2$height ~ d2$weight)
```

### 4.4.1 The linear model strategy

* The strategy is to make the parameter for the mean int oa linear function of the predictor(s).
* For our height example:

$$
h_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta(x_i - \overline{x}) \\
\alpha \sim Normal(178, 20) \\
\beta \sim Normal(0, 10) \\
\sigma \sim Uniform(0, 50)
$$

#### 4.4.1.1 Probability of the data 

* Read $h_i$ and $\mu_i$ as "each $h$" and "each $\mu$. 
* The mean of each now depends on unique values on each row $i$.

#### 4.4.1.2 Linear model

* $\mu_i$ is now *deterministic*, because it described by other parameters.
* We're asking two things in this regression:

1. What is the expected height when $x_i = \overline{x}$? The parameter $\alpha$ answers this question --- in this case $\alpha$ is a centered intercept.
2. What is the change in expected height when $x_i$ changes by 1 unit? The parameter $\beta$ answers this question --- in this case the slope.

#### 4.4.1.3 Priors

* Let's check if the priors are reasonable:

```{r}
set.seed(2971)
N <- 100
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)

plot(NULL,
     xlim = range(d2$weight),
     ylim = c(-100, 400),
     xlab = "weight", ylab = "height")

abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0, 10)")
xbar <- mean(d2$weight)
for (i in 1:N) curve(a[i] + b[i]*(x - xbar),
                     from = min(d2$weight),
                     to = max(d2$weight),
                     add = TRUE,
                     col = col.alpha("black", 0.2))
```

* This is an unreasonable prior!
* No one is shorter than 0 cm and there are not so many people who are taller than the world's tallest person (272 cm).
* A log-normal prior may be more reasonable.

```{r}
b <- rlnorm(1e4, 0, 1)
dens(b, xlim = c(0, 5), adj = 0.1)
```

```{r}
set.seed(2971)
N <- 100
a <- rnorm(N, 178, 20)
b <- rlnorm(N, 0, 1)

plot(NULL,
     xlim = range(d2$weight),
     ylim = c(-100, 400),
     xlab = "weight", ylab = "height")

abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0, 10)")
xbar <- mean(d2$weight)
for (i in 1:N) curve(a[i] + b[i]*(x - xbar),
                     from = min(d2$weight),
                     to = max(d2$weight),
                     add = TRUE,
                     col = col.alpha("black", 0.2))
```

* This is a much more reasonable prior. 
* One thing to note --- adjusting a prior in light of the observed sample just to get some desired result is the Bayesian equivalent of p-hacking.
* In this example, we didn't set a prior based on comparing to the data, but rather used our *prior knowledge* about what reasonable height/weight relationships might look like.

### 4.4.2 Finding the posterior distribution. 

* An update to the model:

$$
h_i \sim Normal(\mu_i, \sigma)\\
\mu_i = \alpha + \beta(x_i - \overline{x}) \\
\alpha \sim Normal(178, 20) \\
\beta \sim LogNormal(0, 1) \\
\sigma \sim Uniform(0, 50)
$$

```{r}
m4.3 <-
  quap(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b*(weight - xbar),
      a ~ dnorm(178, 20),
      b ~ dlnorm(0, 1),
      sigma ~ dunif(0, 50)
    ),
    data = d2
  )
```

### 4.4.3 Interpreting the posterior distribution

* There are two broad categories of processing the posterior:
  1. Reading tables
  2. Plotting simulations
* Most models are very hard to understand from tables of numbers alone.
* Plotting allows you to inquire about things that are hard to learn from tables:
  1. Whether or not the model fitting process worked correctly
  2. The *absolute* magnitude (rather than the *relative* magnitude) of a relationship between an outcome and predictor.
  3. The uncertainty surrounding an average relationship.
  4. The uncertainty surrounding the implied predictions of the model.

#### 4.4.3.1 Tables of marginal distributions

```{r}
precis(m4.3)
```

* Look at `b` --- this can be read as *a person 1 kg heavier is expected to be 0.90 cm taller*. 
* 89% of the posterior probability lies between 0.84 and 0.97 --- a relationship close to 0 or above 1 is highly implausible given these data and this model.
* In this case, there is very little correlation amongst the parameters:

```{r}
round(vcov(m4.3), 3)
pairs(m4.3)
```

#### 4.4.3.2 Plotting posterior inference against the data

* Plotting against the data helps interpret the posterior and provides an informal check on model assumptions.

```{r}
plot(height ~ weight, data = d2, col = rangi2)

# get maximum a posteriri estimates for a/b
post <- extract.samples(m4.3)
a_map <- mean(post$a)
b_map <- mean(post$b)

# add map line ot plot
curve(a_map + b_map*(x - xbar), add = TRUE)
```

#### 4.4.3.3 Adding uncertainty around the mean

* The above plot is a single line defined by $\alpha$ & $\beta$. Let's add more

```{r}
post[1:5, ]
```

```{r}
plot_n_samples <- function(N) {
  
  # filter to just the first N samples in d2
  dN <- d2[1:N, ]
  
  # create a new model
  mN <- 
    quap(
      alist(height ~ dnorm(mu, sigma),
            mu <- a + b*(weight - mean(weight)),
            a ~ dnorm(178, 20),
            b ~ dlnorm(0, 1),
            sigma ~ dunif(0, 50)),
      data = dN
    )
  
  # extract 20 samples from the posterior
  post <- extract.samples(mN, n = 20)
  
  # display raw data & sample size
  plot(dN$weight,
       dN$height,
       xlim = range(d2$weight),
       ylim = range(d2$height),
       col = rangi2,
       xlab = "weight",
       ylab = "height")
  
  mtext(concat("N = ", N))
  
  # plot the lines with transparency
  for (i in 1:20)
    curve(post$a[i] + post$b[i] * (x - mean(dN$weight)),
          col = col.alpha("black", 0.3),
          add = TRUE)
  
}

plot_n_samples(10)
plot_n_samples(50)
plot_n_samples(150)
plot_n_samples(352)
```

#### 4.4.3.4 Plotting regression intervals and contours

* We can also plot uncertainty around the mean estimate for individual points (here's a weight of 50kg as an example):

```{r}
# get posterior estimates of mean height based on a weight of 50kg
mu_at_50 <- post$a + post$b * (50 - xbar)

# plot posterior
dens(mu_at_50, 
     col = rangi2,
     lwd = 2,
     xlab = "mu | weight = 50")

PI(mu_at_50)
```

* For all points, we can use `link()`:

```{r}
mu <- link(m4.3)
str(mu)
```


