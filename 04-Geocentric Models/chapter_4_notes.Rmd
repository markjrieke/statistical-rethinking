---
title: "Geocentric Models"
output: github_document
math: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 500,
  fig.width = 9,
  fig.height = 6
)
```

* Ptolomey's geocentric model of the solar system using epicycles could provide useful information about the future location of the planets, but was wrong.
* *Linear regression* is our geocentric golem --- it's often not necessarily correct, but can be useful.

## 4.1 Why normal distributions are normal

### 4.1.1 Normal by addition

* Let's simulate 16 random steps forward/backward for 1000 people.
* Even though these are effectively coin flips, we'll end up with a normal-ish distribution of of final position at step 16 for these 1000 people.

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))

hist(pos)
plot(density(pos))
```

* Any process that adds together random values from the same distribution converges to a normal distribution.

### 4.1.2 Normal by multiplication

* Let's the growth rate of an organism is determined by a a dozen loci that interact, such that each increase growth by a percentage. This means the effects multiply. Here's an example of 1:

```{r}
prod(1 + runif(12, 0, 0.1))
```

* Now let's repeat this for 10,000 organisms:

```{r}
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
rethinking::dens(growth, norm.comp = TRUE)
```

* This is because small multiplications are approximately additive (e.g., 1.1 * 1.1 = 1.21)

```{r}
big <- replicate(10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))

rethinking::dens(big)
rethinking::dens(small)
```

### 4.1.3 Normal by log-multiplication

* Large deviates that are multiplied together don't produce gaussian distributions, but they do generate log-normal distributions

```{r}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
rethinking::dens(log.big)
```

### 4.1.4 Using Gaussian distributions

* There are two justifications for using Gaussian distributions:
  - **Ontological**: The world is full of Gaussian distributions. Other members of the *Exponential Family* of distributions (Gamma, Poisson) also arise in nature. 
  - **Epistemological**: When all we know or are willing to say about a distribution is their mean and variance, then ghe Gaussian is the most natural way to express this. This is also justified with information theory via measures of *maximum entropy*.
  
## 4.2 A language for describing models

1. We have a set of variables to work with. Some of these are observable --- we call these *data*. Others are unobservable --- we call these *parameters*. 
2. We define each variable either in terms of other variables or in terms of a probability distribution.
3. The combination of variables and their probability distributions defines a *joint generative model*. 

* The biggest difficulty in working with this framework is the subject matter --- which variables matter and how does theory tell us to connect them?

### 4.2.1 Re-describing the globe tossing model

$$
\begin{align*}
W \sim Binomial(N, p) \\
\\
p \sim Uniform(0, 1)\\
\end{align*}
$$

* Both lines in the above model are *stochastic* --- that is no single variable on the left is known with certainty, but rather is described probabilistically. 

## 4.3 Gaussian model of height

### 4.3.1 The data

```{r}
library(rethinking)
data("Howell1")
d <- Howell1

str(d)
```

```{r}
precis(d)
```

```{r}
d2 <- d[d$age >= 18,]
```

### 4.3.2 The model

```{r}
hist(d2$height)
```

* Can model height using a normal distribution
* Notation note: *iid* = *independent and identically distributed*. This assumption is often wrong (for example, heights amongst family members are not independent), but also often useful.

$$
h_{i} \sim Normal(\mu, \sigma) \\
\mu \sim Normal(178, 20) \\
\sigma \sim Uniform(0, 50)
$$

```{r}
# ***average*** height somewhere between 140 & 220 cm
curve(dnorm(x, 178, 20), 
      from = 100,
      to = 250)
```

```{r}
curve(dunif(x, 0, 50),
      from = -10,
      to = 60)
```

```{r}
# prior predictive simulation
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

* Prior prediction is useful for setting reasonable priors. 
* For example, if the prior for the mean, $\mu$, was $\mu \sim Normal(178, 100)$, we'd end up with implausible (negative!) prior heights:

```{r}
# this set of priors is unreasonable!
sample_mu <- rnorm(1e4, 178, 100)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
```

* In this particular case, we have enough data and a simple enough model that a sill prior is harmless. But that won't always be the case. 

### 4.3.3 Grid approximation of the posterior distribution

* For posterity, let's generate the analytical solution to the posterior (won't be useful in the long term, but approximations will be far quicker/less computationally expensive/just as good for practical purposes):

```{r}
# generate grid of mu/sigma
mu.list <- seq(from = 150, to = 160, length.out = 100)
sigma.list <- seq(from = 7, to = 9, length.out = 100)

# generate every combination of mu/sigma (10000 total)
post <- expand.grid(mu = mu.list, sigma = sigma.list)

# generate log-likelihood of data based on each combination of mu/sigma
post$LL <- 
  sapply(1:nrow(post),
         function(i) sum(
           dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE)
         ))

# compute the log product of the likelihood & prior
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)

# find the posterior, convert from log to response scale
post$prob <- exp(post$prod - max(post$prod))
```

```{r}
contour_xyz(post$mu, post$sigma, post$prob)
```

```{r}
image_xyz(post$mu, post$sigma, post$prob)
```

### 4.3.4 Sampling from the posterior

* Let's sample parameter values from the posterior distribution

```{r}
# sample.rows = row indices --- rows with greater plausibility are more likely 
# to be sampled
sample.rows <- 
  sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)

sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]

plot(sample.mu,
     sample.sigma,
     cex = 1,
     pch = 16,
     col = col.alpha(rangi2, 0.1))
```

```{r}
hist(sample.mu)
hist(sample.sigma)
```

```{r}
PI(sample.mu)
PI(sample.sigma)
```

* One interesting thing to note is that $\sigma$ is *right-skewed*. 
* This *basically* has to do with the fact that it must be positive --- if variance is estimated to be near zero, then it can't be much smaller, but could be a lot bigger.

```{r}
# using a sample of 20 points will display this issue further
d3 <- sample(d2$height, size = 20)

# repeat analysis from above, this time looking at only 20 points
# priors:
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)

# combine all possible combinations
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)

# log likelihood
post2$LL <- 
  sapply(
    1:nrow(post2),
    function(i) 
      sum(dnorm(d3, mean = post2$mu[i], sd = post2$sigma[i], log = TRUE))
  )

# log product of likelihood & prior
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) + dunif(post2$sigma, 0, 50, TRUE)

# log posterior converted to response scale
post2$prob <- exp(post2$prod - max(post2$prod))

# sample from the posterior
sample2.rows <- sample(1:nrow(post2), size = 1e4, replace = TRUE, prob = post2$prob)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]

plot(sample2.mu, 
     sample2.sigma, 
     cex = 1, 
     col = col.alpha(rangi2, 0.1),
     xlab = "mu",
     ylab = "sigma",
     pch = 16)
```

```{r}
dens(sample2.sigma, norm.comp = TRUE)
```

### Finding the posterior distribution with `quap`

* `quap()` will allow us to make a *quadratic approximation* of the posterior.
* `quap()` will find the peak at the *maximum a posteriori* estimate (MAP), then approximate the posterior using the curvature at the MAP.
* This is similar to what many non-Bayesian procedures do, just without any priors.
* `quap()` allows us to define the formulas very similarly to the mathematical syntax:

$$
h_{i} \sim Normal(\mu, \sigma) \\
\mu \sim Normal(178, 20) \\
\sigma \sim Uniform(0, 50)
$$

```{r}
flist <-
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 20),
    sigma ~ dunif(0, 50)
  )
```

* Very simple to fit the model to the data:

```{r}
m4.1 <- quap(flist, data = d2)

precis(m4.1)
```

* The table from `precis()` provides the Gaussian approximations for each parameter's *marginal* distribution. 
* This means the plausibility of each value of $\mu$ after averaging over the plausibilities of each value of $\sigma$ is given by a Gaussian distribution with mean 154.6 & std. dev of 0.4.
* `quap()` will start at random parameter values by default, but you can also give it an explicit starting value:

```{r}
start <- 
  list(
    mu = mean(d2$height),
    sigma = sd(d2$height)
  )

# use start values specified above
m4.1 <- quap(flist, data = d2, start = start)

# quap still finds pretty much the same estimates:
precis(m4.1)
```

* Previous priors were very weak --- we can also specify very stron priors:

```{r}
m4.2 <-
  quap(
    alist(
      height ~ dnorm(mu, sigma),
      mu ~ dnorm(178, 0.1), # very strong prior!!!
      sigma ~ dunif(0, 50)
    ),
    data = d2
  )

precis(m4.2)
```

* Strong priors regularize *a lot* so the estimate for $\mu$ has hardly moved. 
* Even though $\mu$ hasn't moved, $\sigma$ has *had* to move quite a bit to compensate. 

### Sampling from a `quap`

* Quadratic approximation is just a multidimensional gaussian ($\mu$ and $\sigma$ both contribute a dimension) distribution.
* Just like a mean and standard deviation are enough to describe a one-dimensional Gaussian distribution, a list of means and a matrix of variances and covariances are enough to describe a multidimensional Gaussian distribution.

```{r}
vcov(m4.1)
```

* `vcov()` returns a *variance-covariance* matrix which tells us how each parameter relates to every other parameter in the posterior distribution.
* A variance-covariance matrix can be factored into two elements:
  1. A vector of variances for the parameters.
  2. A correlation matrix that tells us how changes in any parameter lead to correlated changes in the others.
  
```{r}
diag(vcov(m4.1))
cov2cor(vcov(m4.1))
```

* The two element vector returned by `diag()` is a list of variances (if you take the square root, you get the standard deviations that are shown by `precis()`).
* Instead of sampling single values from a Gaussian, we sample vectors of values from a multi-dimensional gaussian.

```{r}
post <- extract.samples(m4.1, n = 1e4)

head(post)
precis(post)
plot(post)
```
















