---
title: "Generalized Linear Madness"
output: github_document
---

* Sciences construct mathematical models of natural processes that are specialized and can fail in precise ways.
* Statistics has to apply to all the sciences, so it generally has much vaguer models that focus on average performance.
* GLMs are powerful tools (especially in combination with DAGs & *do*-calculus for causal inference) but they are descriptions of associations and are not necessarily credible scientific models for natural processes.
* GLMs can also be restrictive --- not everything can be modeled as a linear combination of variables mapped onto a non-linear outcome. 
* In this chapter, we'll go beyond *generalized linear madness* and work through examples in which the scientific context provides a causal model to inform a statistical model.

## 16.1 Geometric people

* Let's suppose a person is shaped like a cylinder (spherical cow!). We can then have a scientific model that predicts the weight of a person based on their volume, rather than a linear regression between height & weight.

### 16.1.1 The scientific model

* The volume of a cylinder is given by:

$$
V = \pi r^2 h
$$

* Let's assume that a person's radius is some constant proportion of their height, $p$. So we can substitute $r = ph$:

$$
\begin{align*}
V & = \pi(ph)^2h \\
V & = \pi p^2h^3
\end{align*}
$$

* Finally, weight is the volume multiplied by a density $k$:

$$
W = kV = k \pi p^2h^3
$$

* This is not a linear model! But that's okay! It has a causal structure, it makes predictions, and we can fit it to data.

### 16.1.2 The statistical model

$$
\begin{align*}
W_i & \sim \text{Log-Normal}(\mu_i, \sigma) \\
\text{exp}(\mu_i) & = k \pi p^2h^3 \\
k & \sim \text{some prior} \\
p & \sim \text{some prior} \\
\sigma & \sim \text{Exponential}(1)
\end{align*}
$$

* Here, $W$ follows a Log Normal distribution, since it must be positive and continuous. 
* $k$ and $p$ are multiplied together directly & therefore we'd say that these are *not identifiable*.
* We can, however, just replace $kp^2$ with a new parameter $\theta$ instead:

$$
\text{exp}(\mu_i) = \pi \theta h_i^3
$$

* This'll give us the same predictions, but will give us a harder time in setting a reasonable prior for $\theta$. 
* $p$ is the ratio $r/h$, so must be greater than 0. It's very likely less than 1 (not many people wider than they are tall!) and probably less than 0.5. Here's a reasonable prior for $p$:

$$
p \sim \text{Beta}(2, 18)
$$

```{r}
rbeta(1e4, 2, 18) |> rethinking::dens()
```

* $k$ is really just a translation in measurement scales (i.e., if weight is measured in kg and volume in cm^3, then $k$ must have units kg/cm^3). 
* One useful trick is to try to get rid of the measurement scales altogether! They're an arbitrary human invention. We can do so by dividing out by a reference value to cancel units --- in this case we'll use the mean. 

```{r}
library(rethinking)

data("Howell1")
d <- Howell1

# scale observed variables
d$w <- d$weight/mean(d$weight)
d$h <- d$height/mean(d$height)
```

* There's nothing special about using the means, we just need to use a reference to cancel out the units. 
* Now let's consider $k$ --- for a person with an average height & weight, we get:

$$
1 = k \pi p^2 1^3
$$

* Since $p < 0.5$, $k$ must be greater than 1. Let's constrain it to be positive with a rough mean of 2:

$$
k \sim \text{Exponential}(0.5)
$$

* Prior-predictive simulation could reveal that we can do better, but this gives us a starting point to model:

```{r}
m16.1 <-
  ulam(
    alist(
      w ~ dlnorm(mu, sigma),
      exp(mu) <- 3.1415926 * k * p^2 * h^3,
      p ~ beta(2, 18),
      k ~ exponential(0.5),
      sigma ~ exponential(1)
    ),
    
    data = d,
    chains = 4,
    cores = 4
  )

precis(m16.1)
```

* The non-identifiability of $p$ and $k$ means that we get a strong correlation in the parameters:

```{r}
pairs(m16.1, pars = c("k", "p"))
```

* The model has to maintain a constant $kp^2$ when $k$ and $p$ shift, at least according to this model.
* We could model $k$ and $p$ as functions of height or age or other parameters. 
* Here's the posterior predictive distribution across the observed height range:

```{r}
h_seq <- seq(from = 0, to = max(d$h), length.out = 30)
w_sim <- sim(m16.1, data = list(h = h_seq))
mu_mean <- apply(w_sim, 2, mean)
w_CI <- apply(w_sim, 2, PI)

plot(d$h, d$w,
     xlim = c(0, max(d$h)),
     ylim = c(0, max(d$w)),
     col = rangi2,
     lwd = 2,
     xlab = "height (scaled)",
     ylab = "weight (scaled)")

lines(h_seq, mu_mean)
shade(w_CI, h_seq)
```

* The model gets the general scaling right --- the fixed exponent of 3 from our theory does a great job.
* There's a poor fit on some of the smaller heights, possibly because $p$ and/or $k$ is different for children.

### 16.1.3 GLM in disguize

* This model is actually a GLM in disguise!

$$
\begin{align*}
\log{w_i} & = \mu_i = \log{(k \pi p^2 h_i^3)} \\
\log{w_i} & = \log{k} + \log{\pi} + 2 \log{p} + 3 \log{h_i}
\end{align*}
$$

* On the log scale, this is just a plane jane linear regression! But our theory gave us the fixed parameters of 2 & 3. 

## 16.2 Hidden minds and observed behavior

* The *inverse problem* is one of the most basic in scientific inference: how to figure out causes from observations.
* Let's look at an inverse problem from developmental psychology --- given some observation about children's behavior, which decision making strategy caused the choice?
* Here's an example in which 629 children from 4-14 saw four other children choose among colored boxes: 3 choose one color, one chooses another, and one is left unchosen.

```{r}
data("Boxes")
precis(Boxes)
```

* The outcome `y` indicates the chosen color: 1 is the unchosen color from the examples, 2 is the majority, and 3 is the minority. 
* `majority_first` indicates if the example children selected the majority color first (1) or the minority color first (0).
* Here's just the proportion of selections:

```{r}
table(Boxes$y) / length(Boxes$y)
```

* Do 45% of children just use the "follow the majority" strategy? Not necessarily! There are many possible strategies that could produce these data. 

### 16.2.1 The scientific model

* Let's think of a generative process, where half of children choose randomly and half follow the majority:

```{r}
set.seed(7)

# 30 random children
N <- 30

# half are random
y1 <- sample(1:3, size = N/2, replace = TRUE)

# half follow the majority
y2 <- rep(2, N/2)

# combine & shuffle
y <- sample(c(y1, y2))

# count the 2s:
sum(y == 2)/N
```

* Over two-thirds chose the majority color, but only half are explicitly following this strategy!
* Let's consider 5 different strategies children might use:
  1. Follow the majority: copy the majority color (2)
  2. Follow the minority: copy the minority color (3)
  3. Maverick: Choose the unchosen color (1)
  4. Random: Choose randomly
  5. Follow first: Copy the color that was demonstrated first (could be 2 or 3!)

### 16.2.2 The statistical model

* There are 5 possible strategies to choose from, but we only need to estimate 4 probabilities, since they must sum to 1.
* We can handle this with a *simplex* and a (weak) Dirichlet prior for the probabilities $p$:

$$
p \sim \text{Dirichlet}([4, 4, 4, 4, 4])
$$

```{r}
gtools::rdirichlet(1e4, c(4, 4, 4, 4, 4))[,1] |> dens()
```

* The probability of any individual choice is the probability of choosing said option under each of the possible strategies:

$$
\Pr(y_i) = \sum_{s=1}^5 p_s \Pr(y_i |s)
$$

* This is just the mathy way of saying that the probability of $y_i$ is weighted average of the probability based on each strategy $s$.
* Now, altogether in a statistical model:

$$
\begin{align*}
y_i & \sim \text{Categorical}(\theta) \\
\theta_j & = \sum_{s=1}^5 p_s \Pr(j|s) \\
p & \sim \text{Dirichlet}([4, 4, 4, 4, 4])
\end{align*}
$$

* Since we only have one observation per child here, we can't really doo too much better than this. But if we did, we could assign a unique simplex $p$ to each child

### 16.2.3 Coding the statistical model

* We'll write this model directly in Stan, because it's actually easier to both code and extend.
* Previously, Stan has been optional, but now it's really not!

```{r}
# for clarity's sake, phi below refers to the Pr(j|s) above
data("Boxes_model")
cat(Boxes_model)
```

* Let's walk through the stan code:
  1. Observed data are declared in the *data block*. We also need to declare types and lengths.
  2. The *parameters* block is similar to the data block, but for unobserved parameters. Declaring p as a `simplex` in Stan handles the probability conversion for us. 
  3. The *model block* is where the work happens. We declare `phi` to hold the probability of observing the chosen color based on each strategy --- $\Pr(j|s)$ in the model. We also set a prior for `p`, then loop through all row. For each row `i` we calculate the log probability of the observed `y[i]`. Then, the `p` parameters are included by adding the log probabilities together. See pages 535-536 for more detail in the overthinking box. 

```{r}
# prep data
dat_list <-
  list(
    N = nrow(Boxes),
    y = Boxes$y,
    majority_first = Boxes$majority_first
  )

# sample
m16.2 <-
  stan(
    model_code = Boxes_model,
    data = dat_list,
    chains = 3,
    cores = 3
  )

# marginal posterior
p_labels <- c("1 Majority", "2 Minority", "3 Maverick", "4 Random", "5 Follow First")
precis_plot(precis(m16.2, 2), labels = p_labels)
```

* We can also include a model for gender or age:

```{r}
data("Boxes_model_gender")
data("Boxes_model_age")

cat(Boxes_model_gender)
cat(Boxes_model_age)
```




