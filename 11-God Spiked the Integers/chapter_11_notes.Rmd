---
title: "God Spiked the Integers"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 500,
  fig.width = 9,
  fig.height = 6
)
```

* *Generalized Linear Models* (GLMs) are a lot like early mechanical computers --- the moving pieces within (the parameters) interact to produce non-obvious predictions. 
* Understanding the parameters in GLMs will always involve more work than for Gaussian models, because of the transformation on the output scale.
* GLMs let us model counts --- two of the most common types of count models are *Binomial Regression*, which is useful for binary classification, and *Poisson Regression*, which is a special case of the binomial.

## 11.1 Binomial regression

$$
\begin{gather}
y \sim Binomial(n, p)
\end{gather}
$$

* The binomial distribution is denoted above, where $y$ is a count on $[\ 0, \infty)$, $p$ is the probability that any particular "trial" is a success, and $n$ is the number of trials. 
* Two common flavors of binomial models are:
  1. *Logistic Regression* --- for single trial cases, when the outcome can only be 0 or 1
  2. *Aggregated binomial regression* --- for multi-trial cases, where the outcome can be any integer between 0 and $n$
  
### 11.1.1 Logistic regression: Prosocial chimpanzees

* Consider an experiment where we want to test the social tendencies of chimpanzees. In the setup, the focal chimpanzee can pull a lever on the left to deliver food to himself or pull a lever on the right to deliver food to himself an another chimpanzee (see figure 11.2 on page 326). 
* If we set a variable to 1 when the chimpanzee pulls the left trigger, we can model this with a binomial model.

```{r}
library(rethinking)
data("chimpanzees")
d <- chimpanzees

str(d)
```

* We want to infer what happens in each combination of `prosoc_left` and `condition`:
  1. `prosoc_left = 0` and `condition = 0` : two food items on the right and no partner
  2. `prosoc_left = 1` and `condition = 0` : two food items on the left and no partner
  3. `prosoc_left = 0` and `condition = 1` : two food items on the right and partner present
  4. `prosoc_left = 1` and `condition = 1` : two food items on the left and partner present

```{r}
d$treatment <- 1 + d$prosoc_left + 2 * d$condition
xtabs( ~ treatment + prosoc_left + condition, d)
```

* Now each combination is an index in treatment. In mathematical form:

$$
\begin{gather}
L_i \sim Binomial(1, p_i) \\
logit(p_i) = \alpha_{ACTOR[i]} + \beta_{TREATMENT[i]} \\
\alpha_j \sim to \ be \ determined \\
\beta_k \sim to \ be \ determined 
\end{gather}
$$

* Here, $L$ is the 0/1 variable for `pulled_left` and $\alpha_j$ is a parameter for each of the 7 chimpanzees. Alternatively, this could have been defined with a Bernoulli distribution:

$$
\begin{gather}
L_i \sim Bernoulli(p_i)
\end{gather}
$$

* The TBD priors are a bit weird to work with for GLMs --- let's start off with a super simple example:

$$
\begin{gather}
L_i \sim Binomial(1, p_i) \\
logit(p_i) = \alpha \\
\alpha \sim Normal(0, \omega)
\end{gather}
$$

* We'll change up $\omega$ to see what happens. To start, we'll illustrate the madness of flat priors with $\omega = 10$.

```{r}
# model with wide prior
m11.1 <-
  quap(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a,
          a ~ dnorm(0, 10)),
    data = d
  )

# sample from the prior
set.seed(1999)
prior <- extract.prior(m11.1, n = 1e4)
p <- inv_logit(prior$a) # need to get the prior on the outcome scale!

# wonk!
dens(p, adj = 0.1)
```

* A flat prior in the logit space is not a flat prior in the probability space!
* A *slightly* more regularized prior with $\omega = 1.5$ is more akin to what we want:

```{r}
# model with better prior
m11.1a <-
  quap(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a,
          a ~ dnorm(0, 1.5)),
    data = d
  )

# sample from the prior
set.seed(1999)
prior <- extract.prior(m11.1a, n = 1e4)
p <- inv_logit(prior$a) # need to get the prior on the outcome scale!

# wonk!
dens(p, adj = 0.1)
```

* Now let's do the same weirdness with unconventionally flat priors for the $\beta$ parameters, just to drive this home.

```{r}
# model with bad priors on treatment
m11.2 <-
  quap(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a + b[treatment],
          a ~ dnorm(0, 1.5),
          b[treatment] ~ dnorm(0, 10)),
    data = d
  )

# extract priors
set.seed(1999)
prior <- extract.prior(m11.2, n = 1e4)
p <- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k]))

# plot the *difference* between treatments 1 and 2:
dens(abs(p[,1] - p[,2]), adj = 0.1)
```

* Again, the flat prior means that the model believes that the treatments are either completely alike or completely different, which is not necessarily what we think!
* Changing to a $\beta_j \sim Normal(0, 0.5)$ prior results in priors outcome that a difference of 0 is the highest prior probability, and the average is about 10%

```{r}
# model with better priors for alpha and beta
m11.3 <-
  quap(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a + b[treatment],
          a ~ dnorm(0, 1.5),
          b[treatment] ~ dnorm(0, 0.5)),
    data = d
  )

# extract priors
set.seed(1999)
prior <- extract.prior(m11.3, n = 1e4)
p <- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k]))

# return the average prior difference between treatments 1 and 2:
mean(abs(p[,1] - p[,2]))

# plot!
dens(abs(p[,1] - p[,2]), adj = 0.1)
```

* This makes sense, because it's the *difference* in treatment effect.
* Each treatment runs relatively evenly between 0 and 1, so *on average* the difference is likelier to be small (it's more likely that each is somewhere in the middle rather than being on opposite ends). 

```{r}
# prep data for ulam
dat_list <-
  list(
    pulled_left = d$pulled_left,
    actor = d$actor,
    treatment = as.integer(d$treatment)
  )

# model! 
# log_lik = TRUE will have ulam() compute the values necessary for PSIS/WAIC
m11.4 <-
  ulam(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a[actor] + b[treatment],
          a[actor] ~ dnorm(0, 1.5),
          b[treatment] ~ dnorm(0, 0.5)),
    data = dat_list,
    chains = 4,
    log_lik = TRUE
  )

# posterior parameter estimations
precis(m11.4, depth = 2)

# plot parameters:
post <- extract.samples(m11.4)
p_left <- inv_logit(post$a)
precis_plot(precis(as.data.frame(p_left)), xlim = c(0, 1))
```

* Here each row is a chimpanzee. Chimps 1, 3, 4, and 5 show a preference for the right lever. 2 and 7 show a preference for the left, with 2 preferring it greatly.
* Having repeat measurements per actor (chimp), is useful for subtracting out this chimp-level preference to isolate the treatment effect.

```{r}
# plot posterior treatment
labs <- c("R/N", "L/N", "R/P", "L/P")
precis_plot(precis(m11.4, depth = 2, pars = "b"), labels = labs)
```

* Here, "L/N" means "prosocial on left, no partner" and "R/P" means "prosocial on right/partner."
* We're looking to see if chimpanzees choose the prosocial option when a partner is present, so we ought to compare the first/third row and the second/fourth row:

```{r}
diffs <-
  list(
    db13 = post$b[,1] - post$b[,3],
    db24 = post$b[,2] - post$b[,4]
  )

precis_plot(precis(diffs))
```

* Here, there is weak evidence of the pulling the prosocial choice on the right when the partner is present, but the interval is pretty wide. 
* There is pretty negligible evidence for pulling the prosocial choice when the option is on the left (if anything, they choose the prosocial option when the partner is absent!)
* Let's compare the proportion each chimp actually pulled the left lever with the model's posterior predictions.

```{r}
pl <- by(d$pulled_left, list(d$actor, d$treatment), mean)

# proportion of pulling left under each treatment for chimp 1:
pl[1,]
```

```{r}
# plot proportions:
plot(NULL, 
     xlim = c(1, 28),
     ylim = c(0, 1),
     xlab = "",
     ylab = "proportion left lever",
     xaxt = "n",
     yaxt = "n")

axis(2, 
     at = c(0, 0.5, 1),
     labels = c(0, 0.5, 1))

abline(h = 0.5,
       lty = 2)

for (j in 1:7) abline(y = (j - 1)*4 + 4.5, lwd = 0.5)
for (j in 1:7) text((j - 1)*4 + 2.5, 1.1, concat("actor ", j), xpd = TRUE)
for (j in (1:7)[-2]) {
  lines((j - 1)*4 + c(1, 3), pl[j, c(1, 3)], lwd = 2, col = rangi2)
  lines((j - 1)*4 + c(2, 4), pl[j, c(2, 4)], lwd = 2, col = rangi2)
}

points(1:28, t(pl), pch = 16, col = "white", cex = 1.7)
points(1:28, t(pl), pch = c(1, 1, 16, 16), col = rangi2, lwd = 2)

yoff <- 0.01
text(1, pl[1, 1] - yoff, "R/N", pos = 1, cex = 0.8)
text(2, pl[1, 2] + yoff, "L/N", pos = 3, cex = 0.8)
text(3, pl[1, 3] - yoff, "R/P", pos = 1, cex = 0.8)
text(4, pl[1, 4] + yoff, "L/P", pos = 3, cex = 0.8)
```

```{r}
# posterior predictions
# see figure 11.4 on page 333 for the actual plot lol
dat <- list(actor = rep(1:7, each = 4), treatment = rep(1:4, times = 7))
p_post <- link(m11.4, data = dat)
p_mu <- apply(p_post, 2, mean)
p_ci <- apply(p_post, 2, PI)
```

* There doesn't seem to be any much evidence of the prosocial option being chosen more when a partner is present, but to test it out explicitly, let's build a model with an interaction effect.
* The simpler model will likely do just fine, because we don't expect to see an interaction:

```{r}
# prep new vars
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

# prep new list for ulam
dat_list2 <-
  list(pulled_left = d$pulled_left,
       actor = d$actor,
       side = d$side,
       cond = d$cond)

# model
m11.5 <- 
  ulam(
    alist(pulled_left ~ dbinom(1, p),
          logit(p) <- a[actor] + bs[side] + bc[cond],
          a[actor] ~ dnorm(0, 1.5),
          bs[side] ~ dnorm(0, 0.5),
          bc[cond] ~ dnorm(0, 0.5)),
    data = dat_list2,
    chains = 4,
    log_lik = TRUE
  )

# finally, compare models w/PSIS
compare(m11.5, m11.4, func = PSIS)
```

* The modle comparison here is for the sake of understanding, but the experiment + hypothesis tell us which model to use (`m11.4`), this comparison just lets us know that `m11.5` doesn't add a whole lot. 

```{r}
# follow along with the overthinking box on pages 334-336
post <- extract.samples(m11.4, clean = FALSE) # clean = FALSE returns the log-probability
str(post)

stancode(m11.4)
```

### 11.1.2 Relative shark and absolute deer

* The above focused on the posterior difference treatment made on the outcome scale --- this focuses on *absolute effects*. 
* Oftentimes, we're interested in the *relative effect* a term has in a logistic model --- the proportional change in the odds of an outcome. 
* We can calculate these *proportional odds* relative effect sizes by exponentiating the parameter of interest.

```{r}
post <- extract.samples(m11.4)

# what is the proportional odds difference between treatments 4 & 2?
mean(exp(post$b[,4] - post$b[,2]))
```

* On average, the switch from treatment 2 to treatment 4 (adding a partner) multiplies the odds of pulling the left lever by 0.92 (an 8% reduction in odds). 
* The risk in proportional odds is that they don't necessarily tell us if a parameter is important or not --- for example if the odds of some event are 1 in 1 million, and a parameter change involves a 5.0 proportional odds increase, then the resulting odds now only 5 in 1 million.
* Relative and absolute risks are both important in --- consider relative sharks and absolute deer. People are very afraid of sharks but not so much of deer --- sharks themselves are more dangerous *relative* to deer, but more people are killed by deer each year so deer are more *absolutely* dangerous.

### 11.1.3 Aggregated binomial: Chimpanzees again, condensed

* The original model treated each pull of the lever as a separate trial, but we could aggregate the trials by chimp:

```{r}
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right = 1, left = 2
d$cond <- d$condition + 1 # no partner 1, partner 2

d_aggregated <- 
  aggregate(
    d$pulled_left,
    list(treatment = d$treatment,
         actor = d$actor,
         side = d$side,
         cond = d$cond),
    sum
  )

colnames(d_aggregated)[5] <- "left_pulls"

d_aggregated
```

* We can redefine the model in these aggregated terms & get the same inferences as before:

```{r}
# prep new aggregated data for ulam
dat <- 
  with(
    d_aggregated,
    list(left_pulls = left_pulls,
         treatment = treatment,
         actor = actor,
         side = side,
         cond = cond)
  )

# model the aggregated data!
m11.6 <- 
  ulam(
    alist(left_pulls ~ dbinom(18, p), # each chimp has 18 pulls
          logit(p) <- a[actor] + b[treatment],
          a[actor] ~ dnorm(0, 1.5),
          b[treatment] ~ dnorm(0, 0.5)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )

# compare!
compare(m11.6, m11.4, func = PSIS)
```

* Here, PSIS shows very different values, even though they're effectively the same. This is just because `dbinom()` contains the multiplicity term for all the possible orders the successes could appear (for example, here's 6 successes in 9 trials):

$$
\begin{gather}
Pr(6|9, p) = \frac{6!}{6!(9 - 6)!} \ p^6 (1 - p)^{9 - 6}
\end{gather}
$$

* When all the trials are split into 9 different 0/1 trials, there is no multiplicity term. So the joint probability for the aggregated data is larger --- there are more ways to see the data --- and the PSIS/WAIC scores end up smaller.

```{r}
# deviance of aggregated 6-in-9
-2*dbinom(6, 9, 0.2, log = TRUE)

# deviance of disaggregated
-2*sum(dbern(c(1, 1, 1, 1, 1, 1, 0, 0, 0), 0.2, log = TRUE))
```

* This difference is meaningless --- it's just a side effect of how the data is organized. Posterior inference will be the same!
* We do, however, get a warning about Pareto *k* values --- why didn't we get this warning for the diaggregated data? 
* This is because of WAIC/PSIS are calculated with loo, but when there are multiple (18) trials per observation, it's more like leave 18 out! So we get a warning for highly influential observations.
* Long story short --- if you want to calculate WAIC or PSIS, you should use a *logistic regression* format, otherwise you implicitly assume only large chunks of data are separable (though there are times where this makes sense, like multilevel models!).

### 11.1.4 Aggregated binomial: Graduate schoold admissions

* In the chimpanzee case, the aggregated data had the same number of trials for each observation, but this often isn't the case.
* This can be handled simply by inserting a variable for the number of trials!

```{r}
data("UCBadmit")
d <- UCBadmit

d
```

* Our job here is to evaluate whether the data contains evidence of gender bias in admissions. Here's what we'll model:

$$
\begin{gather}
A_i \sim Binomial(N_i, p_i) \\
logit(p_i) = \alpha_{GID[i]} \\
\alpha_j \sim Normal(0, 1.5)
\end{gather}
$$

```{r}
# prep data for ulam
dat_list <-
  list(admit = d$admit,
       applications = d$applications,
       gid = ifelse(d$applicant.gender == "male", 1, 2))

# model!
m11.7 <- 
  ulam(
    alist(admit ~ dbinom(applications, p),
          logit(p) <- a[gid],
          a[gid] ~ dnorm(0, 1.5)),
    data = dat_list,
    chains = 4
  )

precis(m11.7, depth = 2)
```

* There certainly seems to be a difference! To get better insight, let's compare on the logit scale (relative shark!) as well as on the outcome (absolute deer!)

```{r}
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a = diff_a, diff_p = diff_p))
```

* Here, the log-odds difference is certainly positive, and the difference (in favor of males) is somewhere between 12% and 16% on the probability scale. 
* However, our predictions aren't very good!

```{r}
postcheck(m11.7)

# draw lines connecting points from the same dept
for (i in 1:6) {
  
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x + 1]/d$applications[x + 1]
  lines(c(x, x+1), c(y1, y2), col = rangi2, lwd = 2)
  text(x + 0.5, (y1 + y2)/2 + 0.05, d$dept[x], cex = 0.8, col = rangi2)
  
}
```

* Sometimes bad fits are the result of coding mistakes, but this isn't. The model is correctly answering the bad question, *what are the average probabilities of admission for men and women across all departments?*
* We instead want to answer the question, *what is the average difference in probability of admission between men and women within departments?*

$$
\begin{gather}
A_i \sim Binomial(N_i, p_i) \\
logit(p_i) = \alpha_{GID[i]} + \delta_{DEPT[i]} \\
\alpha_j \sim Normal(0, 1.5) \\ 
\delta_k \sim Normal(0, 1.5)
\end{gather}
$$

```{r}
# create an id for each department
dat_list$dept_id <- rep(1:6, each = 2)

# model!
m11.8 <-
  ulam(
    alist(admit ~ dbinom(applications, p),
          logit(p) <- a[gid] + delta[dept_id],
          a[gid] ~ dnorm(0, 1.5),
          delta[dept_id] ~ dnorm(0, 1.5)),
    data = dat_list,
    chains = 4,
    iter = 4000
  )

precis(m11.8, depth = 2)
```

* Now the intercept for male applicants, `a[1]`, is now a little smaller than the female applicants. Let's calculate the contrasts again on both scales:

```{r}
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis(list(diff_a = diff_a, diff_p = diff_p))
```

* Now we see that if male applicants have it worse, it's only by a seemingly little bit (~ 2% on average).
* Adding in departments changed the inference about gender because the rates of admission vary quite a bit across departments:

```{r}
pg <- 
  with(
    dat_list, 
    sapply(1:6, function(k) applications[dept_id == k]/sum(applications[dept_id == k]))
  )

rownames(pg) <- c("male", "female")
colnames(pg) <- unique(d$dept)
round(pg, 2)
```

* Department is both a confound and legitimate predictor --- in DAG form:

```{r}
library(dagitty)
admit_dag <- 
  dagitty(
    "dag{
      G -> A
      G -> D
      D -> A
    }"
  )

coordinates(admit_dag) <- 
  list(x = c(G = 1, D = 2, A = 3),
       y = c(G = 1, D = 0, A = 1))

drawdag(admit_dag)
```

* Including *D* as a predictor in the model closes the indirect path between *G* and *A*. 
* This is an example of *mediation* analysis. 
* There could still be unobserved confounders, for example, if an unobserved variable for academic ability influences both which department a candidate selects and their admit rate, a DAG might be:

```{r}
admit_dag <- 
  dagitty(
    "dag{
      U[unobserved]
      G -> D
      G -> A
      D -> A
      U -> D
      U -> A
    }"
  )

coordinates(admit_dag) <- 
  list(x = c(G = 1, D = 2, A = 3, U = 3),
       y = c(G = 1, D = 0, A = 1, U = 0))

drawdag(admit_dag)
```

* Finally, `m11.8` is technicall overparametereized --- we don't actually need one of the parameters (either `a[1]` or `a[2]`). Since we just care about the delta, we can use a dummy variable. Though we do see high correlations among the predictors:

```{r}
pairs(m11.8)
```

* On the outcome scale, the posterior predictions are much tighter:

```{r}
postcheck(m11.8)
```

* Over-parameterizing the model like we've done here isn't a violation of any statistical principle (and we get the benefit of assigning priors and not making assumptions about the indicator being more uncertain than baseline), we just need to make sure the model can handle the high correlations.

## 11.2 Poisson regression

* Binomial regression is good for when we know the upper bound of a count distribution, but what about when we don't? For example, if I go fishing and return with 7 fish, what was the maximum? 
* The binomial models still works here! Just with a special condition that results in the Poisson distribution. 
* When a binomial distribution has a small probability $p$ and a large number of trials $N$, it takes on a special shape. The expected value $Np$ is approximately equal to the variance $Np(1 - p)$. 

```{r}
# simulate 100000 samples of 1000 trials of an event with 1/1000 probability
y <- rbinom(1e5, 1000, 1/1000)
c(mean(y), var(y))
```

* A Poisson model can be specified as:

$$
\begin{gather}
y_i \sim Poisson(\lambda_i) \\
log(\lambda_i) = \alpha + \beta(x_i - \overline x)
\end{gather}
$$

* Where $\lambda$ is the mean/variance of the expected count of $y$ and the log link ensures that $\lambda$ is always positive.
* This log link also implies an exponential relationship between the predictors and the expected value --- we need to check that the log link makes sens at all ranges of the predictor variables and that the priors are scaling appropriately.

### 11.2.1 Example: Oceanic tool complexity

* Let's walk through an example of predicting the number of types of tools on historical island populations.

```{r}
data("Kline")
d <- Kline
d
```

* We'll model `total_tools` using the idea that:
  1. The number of tools increases with the log of the population size (from anthropological theory).
  2. The number of tools increases with the contact rate among islands.
  3. The impact of population on tool counts is moderated by high contact (aka, we'll model an interaction between the two). 
  
$$
\begin{gather}
T_i \sim Poisson(\lambda_i) \\
log(\lambda_i) = \alpha_{CID[i]} + \beta_{CID[i]} \ log(P_i) \\
\alpha_j \sim to \ be \ determined \\
\beta_j \sim to \ be \ determined
\end{gather}
$$

* Let's figure out sensible priors. To start, let's consider a vague $Normal(0, 10)$ prior on a single intercept term:

$$
\begin{gather}
T_i \sim Poisson(\lambda_i) \\
log(\lambda_i) = \alpha \\
\alpha \sim Normal(0, 10)
\end{gather}
$$

```{r}
# plot the prior distribution on the outcome scale
curve(dlnorm(x, 0, 10), from = 0, to = 100, n = 200)
```

* The range of number of tools spans from 0-100, like we want, but there is an implausibly large spike at 0 due to our poor prior choice!
* A weakly informative suggestion via McElreath is to use a $\alpha \sim Normal(3, 0.5)$ prior:

```{r}
curve(dlnorm(x, 3, 0.5), from = 0, to = 100, n = 200)
```

* That's much better! Now let's work on a prior for $\beta$ --- for dramatic effect we'll first consider an incredibly wide prior:

```{r}
N <- 100
a <- rnorm(N, 3, 0.5) # reasonable prior for alpha
b <- rnorm(N, 0, 10) # wide prior for beta

# plot!
plot(NULL, xlim = c(-2, 2), ylim = c(0, 100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add = TRUE, col = grau())
```

* The priors on the are unreasonable in that there is explosive growth just beyond the mean or catastrophic decline just after. 
* McElreath settles on a $\beta \sim Normal(0, 0.2)$ prior, though notes that he's tempted from his experience to force the prior to be positive. 

```{r}
# prior simulation for the tighter prior
set.seed(10)
N <- 100
a <- rnorm(N, 3, 0.5)
b <- rnorm(N, 0, 0.2)
plot(NULL, xlim = c(-2, 2), ylim = c(0, 100))
for (i in 1:N) curve(exp(a[i] + b[i]*x), add = TRUE, col = grau())
```

* This new prior still allows for strong relationships, but not as explosive. 
* Let's switch over from the standardized to natural scales for the population:

```{r}
x_seq <- seq(from = log(100), to = log(200000), length.out = 100)
lambda <- sapply(x_seq, function(x) exp(a + b*x))

# total tools vs. log-population
plot(NULL, 
     xlim = range(x_seq), 
     ylim = c(0, 500), 
     xlab = "log population", 
     ylab = "total tools")

for (i in 1:N) lines(x_seq, lambda[i,], col = grau(), lwd = 1.5)

# total tools vs. population
plot(NULL,
     xlim = range(exp(x_seq)),
     ylim = c(0, 500), 
     xlab = "population",
     ylab = "total tools")

for(i in 1:N) lines(exp(x_seq), lambda[i,], col = grau(), lwd = 1.5)
```

* Let's code both the interaction model and the intercept only model:

```{r}
# prep data for passing to ulam
d$P <- scale(log(d$population))
d$contact_id <- ifelse(d$contact == "high", 2, 1)
dat <-
  list(
    T = d$total_tools,
    P = d$P,
    cid = d$contact_id
  )

# intercept only model
m11.9 <- 
  ulam(
    alist(T ~ dpois(lambda),
          log(lambda) <- a,
          a ~ dnorm(3, 0.5)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )

# interaction model
m11.10 <- 
  ulam(
    alist(T ~ dpois(lambda),
          log(lambda) <- a[cid] + b[cid]*P,
          a[cid] ~ dnorm(3, 0.5),
          b[cid] ~ dnorm(0, 0.2)),
    data = dat,
    chains = 4,
    log_lik = TRUE
  )

# compare the models with PSIS
compare(m11.9, m11.10, func = PSIS)
```

* Here we get another Pareto k warning for highly influential points --- this is somewhat expected, given the size of the dataset (n = 10). 
* It's no surprise that the interaction model does better than the intercept model, however it may be surprising that the "effective number of parameters" for the intercept model is *more* than the interaction model!

```{r}
k <- PSIS(m11.10, pointwise = TRUE)$k
plot(dat$P,
     dat$T,
     xlab = "log population (std)",
     ylab = "total tools",
     col = rangi2,
     pch = ifelse(dat$cid == 1, 1, 16), 
     lwd = 2,
     ylim = c(0, 75),
     cex = 1 + normalize(k))

# set up the horizontal axis values to compute predictions at
ns <- 100 
P_seq <- seq(from = -1.4, to = 3, length.out = ns)

# predictions for cid = 1 (low contact)
lambda <- link(m11.10, data = data.frame(P = P_seq, cid = 1))
lmu <- apply(lambda, 2, mean)
lci <- apply(lambda, 2, PI)
lines(P_seq, lmu, lty = 2, lwd = 1.5)
shade(lci, P_seq)

# predictions for cid = 2 (high contact)
lambda <- link(m11.10, data = data.frame(P = P_seq, cid = 2))
lmu <- apply(lambda, 2, mean)
lci <- apply(lambda, 2, PI)
lines(P_seq, lmu, lty = 1, lwd = 1.5)
shade(lci, P_seq)
```

* Here, open points are low contact societies, filled are high contact societies, and the size of the points are scaled by the Pareto k values. 
* Here's the same on the natural scale:

```{r}
plot(d$population,
     d$total_tools,
     xlab = "population",
     ylab = "total tools",
     col = rangi2,
     pch = ifelse(dat$cid == 1, 1, 16), 
     lwd = 2,
     ylim = c(0, 75),
     cex = 1 + normalize(k))

# set up the horizontal axis values to compute predictions at
ns <- 100 
P_seq <- seq(from = -5, to = 3, length.out = ns)
pop_seq <- exp(P_seq * 1.53 + 9) # 9/1.53 are the mean/sd of log(pop)

# predictions for cid = 1 (low contact)
lambda <- link(m11.10, data = data.frame(P = P_seq, cid = 1))
lmu <- apply(lambda, 2, mean)
lci <- apply(lambda, 2, PI)
lines(pop_seq, lmu, lty = 2, lwd = 1.5)
shade(lci, pop_seq)

# predictions for cid = 2 (high contact)
lambda <- link(m11.10, data = data.frame(P = P_seq, cid = 2))
lmu <- apply(lambda, 2, mean)
lci <- apply(lambda, 2, PI)
lines(pop_seq, lmu, lty = 1, lwd = 1.5)
shade(lci, pop_seq)
```

* There are several influential points/islands that break beyond the threshold of Pareto k < 0.5, but Hawaii is the most overtly influential at 1.01 (top right open circle). 
* Poor models would drop Hawaii as an "outlier". We can do better with some careful thought. 
* Look at the dashed, low contact line vs the dark high contact line --- the model currently thinks that (on average) low contact societies at higher populations will have a greater number of tools than high contact societies. From experience, we know this likely isn't true! 
* This is an artifact of the statistical model we've built that allows for an intercept --- at 0 population, there will be *more than 0* tools built. This is a bit nonsensical!
* Let's look at a scientific, rather than statistical, model. Tools develop over time --- innovation adds them to a population while processes of loss remove them. 
* If we assume each person *adds* innovative power to a society with diminishing returns and tool loss is proportional to the number of tools without diminishing returns, we can model the change in the expected number of tools at one time step as:

$$ 
\begin{gather}
\Delta T = \alpha P^\beta - \gamma T
\end{gather}
$$

* Here, $P$ is the population size, $T$ is the number of tools, and $\alpha$, $\beta$, and $\gamma$ are parameters to be estimated. To find the equilibrium number of tools, we just solve for $T$:

$$
\begin{gather}
\hat{T} = \frac{\alpha P^\beta}{\gamma} \\
\end{gather}
$$

* Now, we can place this inside a Poisson model:

$$
\begin{gather}
T_i \sim Poisson(\lambda_i) \\
\lambda_i = \frac{\alpha P_i^\beta}{\gamma}
\end{gather}
$$

* Note that there is no link! All we need to do to ensure a positive $\lambda$ is to make sure the parameters are positive. 
* McElreath sets $\alpha$ to a lognormal prior and $\beta$ and $\gamma$ to exponential priors to do so:

```{r}
dat2 <- 
  list(
    T = d$total_tools,
    P = d$population,
    cid = d$contact_id
  )

m11.11 <-
  ulam(
    alist(T ~ dpois(lambda),
          lambda <- exp(a[cid])*P^b[cid]/g,
          a[cid] ~ dnorm(1, 1),
          b[cid] ~ dexp(1),
          g ~ dexp(1)),
    data = dat2,
    chains = 4,
    log_lik = TRUE
  )
```

* We can replot the posterior predictions on the standardized/natural scale & see that thinking a bit more carefully about the process can generate better predictions (see figure 11.10 on page 355). 






